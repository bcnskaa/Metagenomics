##
# KEGG REST API:
# http://www.kegg.jp/kegg/docs/keggapi.html
#
##
from __future__ import print_function
from __future__ import division

import glob
from itertools import izip
import gc
import os
from collections import Counter
import operator

"""
import process_m8
import glob
import gc
import os


m8_fns = glob.glob("*nr.m8")

#### .map
# Generate .map files
process_m8.process_NR2Seed()

#### .gi2go
# Generate .gi2go
# Import gi2go_map
gi2go_map = process_m8.import_gi2go()
for m8_fn in m8_fns:
    if not os.path.isfile(m8_fn+".gi2go"):
        mm = process_m8.map_m8_gi2go(m8_fn, m8_fn+".gi2go", gi2go_map)


#### merge .map and .gi2go
m8_fns = glob.glob("*nr.m8")
for m8_fn in m8_fns:
    process_m8.merge_gi2go_map(m8_fn+".map", m8_fn+".gi2go", m8_fn + ".gi2go+map", dump_read_id=False)


#### .lineage from .gi2go+map
# Generate .lineage
taxid2lineage_map = process_m8.import_taxid2lineage()
gi2go_map_fns = glob.glob("*.gi2go+map")

for gi2go_map_fn in gi2go_map_fns:
    process_m8.map_lineage2gi2go_map_fn(gi2go_map_fn, taxid2lineage_map=taxid2lineage_map)



#### .mapped2SEED
[peg2function_map, SEED_families2func_map, fig2SEED_families_map, asserted_fig2SEED_families_map, subsystem2role_map] = process_m8.import_seed_data() 
m8_fns = glob.glob("*SEED.m8")
for m8_fn in m8_fns:
    process_m8.process_SEED(m8_fn,peg2function_map, SEED_families2func_map, fig2SEED_families_map, asserted_fig2SEED_families_map, subsystem2role_map)


#### .mapped2SEED.filtered
mapped2SEED_fns = glob.glob("*.mapped2SEED")
for mapped2SEED_fn in mapped2SEED_fns:
    process_m8.filter_mapped2SEED_with_subsystem(mapped2SEED_fn)
    


#### .mapped2SEED.filtered+gi2go+map+lineage

# inside the directory: ~/MG/m8/scaffolds/NR
gi2go_fns = glob.glob("*+nr.m8.gi2go+map+lineage")
for gi2go_fn in gi2go_fns:
    print("Processing " + gi2go_fn)
    #SEED_dir = "../SEED/"
    SEED_dir = "./"
    read_1_fn = gi2go_fn.replace("+nr.m8.gi2go+map+lineage", "+SEED.m8.mapped2SEED.filtered")
    #read_2_fn = gi2go_fn.replace("_1+nr.m8.gi2go+map+lineage", "_2+SEED.m8.mapped2SEED.filtered")
    #read_1_fn = gi2go_fn.replace("+nr.m8.gi2go+map+lineage", "_1.trimmed+SEED.m8.mapped2SEED.filtered")
    #read_2_fn = gi2go_fn.replace("+nr.m8.gi2go+map+lineage", "_2.trimmed+SEED.m8.mapped2SEED.filtered")
    print(read_1_fn)
    #print(read_1_fn + " and " + read_2_fn)
    process_m8.merge_mapped2SEED_gi2go_map(SEED_dir + read_1_fn, gi2go_fn, "./" + read_1_fn + "+gi2go+map+lineage") 
    #process_m8.merge_mapped2SEED_gi2go_map(SEED_dir + read_2_fn, gi2go_fn, "./" + read_2_fn + "+gi2go+map+lineage")
    gc.collect()
    #%reset


"""



"""
m8 format is generated by DIAMOND by using query sequences to search against 
some reference databases such as NCBI NR database. If the m8 format is compatible with
the tabular output format (-outfmt 6) produced by BLAST. If using NR as reference database,
the query sequences can be functionally and taxonomically annotated based on mapping subject sequences.
The functions in this file are prepared for processing the m8 results.

#Usage:

import process_m8

process_m8.process_NR2Seed()

"""
def process_NR2Seed():
    
    largest_gi = 815659664 + 1
    gi2taxid = [0] * (largest_gi + 1)
    gi2seed = [0] * (largest_gi + 1) 
    gi2taxid_map_fn = "/home/siukinng/db/Markers/ncbi_nr/mapping/ncbi_tax/gi_taxid_prot.dmp"
    gi2seed_map_fn="/home/siukinng/db/Markers/ncbi_nr/mapping/gi2seed.map"
    
    
    ## Import gi2seed_map
    print("Importing gi2seed_map from " + gi2seed_map_fn)
    with open(gi2seed_map_fn) as IN:
        s_vals = IN.read().splitlines()
    
    print("Number of rows imported: " + str(len(s_vals)))
    for v in s_vals:
        [gi, seed_id] = map(int, v.split("\t"))
        gi2seed[gi] = seed_id
    
    
    ## Import gi2taxid_map
    print("Importing gi2taxid_map from " + gi2taxid_map_fn)
    with open(gi2taxid_map_fn) as IN:
        vals = IN.read().splitlines()
    
    
    print("Number of rows imported: " + str(len(vals)))
    for v in vals:
        [gi, taxid] = map(int, v.split("\t"))
        gi2taxid[gi] = taxid

    
    #m8_fns = glob.glob("/home/siukinng/MG/m8/scaffolds/*.m8")
    m8_fns = glob.glob("./*nr.m8")
  
    
    for m8_fn in m8_fns:
        out_fn = m8_fn + ".map"
        if os.path.isfile(out_fn):
            continue
        
        print("Processing " + m8_fn)
        with open(m8_fn) as IN:
            m8 = IN.read().splitlines()
        m8_map = [0] * len(m8)
    
    
        #
        skipped_n = 0
        processed_n = 0
        for i, m in enumerate(m8):
            m = m.split("\t")
            # Query ID
            qid = m[0]
            # GI
            gi = int(m[1].split("|")[1])
            tax_id = ""
            seed_id = ""
            try:
                # tax_id
                tax_id = gi2taxid[gi]
                # seed_id
                seed_id = gi2seed[gi]
                processed_n += 1
            except:
                skipped_n += 1
            m8_map[i] = [qid, gi, tax_id, seed_id]
    
        
        print("Exporting results to " + out_fn )
        with open(out_fn , "w") as OUT:
            OUT.write("#read_id\tgi\ttax_id\tseed_id\n")    
            for m in m8_map:
                OUT.write("\t".join(map(str, m)) + "\n")
        print("Number of processed: " + str(processed_n) + " (" + str(skipped_n) + " skipped)")





"""
This function is used to prepare the map for linking GIs to GO terms.
"""
def import_gi2go(gi2go_map_fn="/home/siukinng/db/Markers/Uniprot/idmapping_selected.tab.gi2go"):
    gi2go_map = []
    with open(gi2go_map_fn) as IN:
        gi2go_map = IN.read().splitlines()
    gi2go_map = [g.split("\t") for g in gi2go_map]
    gi2go_map = {int(g[0]):g[1] for g in gi2go_map if len(g) == 2 and len(g[0]) > 0}
    
    return gi2go_map
    


"""
This function is used to parse files in m8 format, and map GIs presented in  
subject IDs to GO terms if exists.

Usage:

import process_m8
import glob
import os

m8_fns = glob.glob("*nr.m8")

# Import gi2go_map
gi2go_map = process_m8.import_gi2go()
for m8_fn in m8_fns:
    if not os.path.isfile(m8_fn+".gi2go"):
        mm = process_m8.map_m8_gi2go(m8_fn, m8_fn+".gi2go", gi2go_map)



"""
def map_m8_gi2go(m8_fn, gi2go_ofn, gi2go_map=None):
    if gi2go_map is None:
        print("Importing GI2GO map...")
        gi2go_map = import_gi2go()
    
    if gi2go_ofn is None:
        gi2go_ofn = m8_fn + ".gi2go"
    
    print("Processing " + m8_fn + " and exporting to " + gi2go_ofn)
    IN = open(m8_fn, "r")
    OUT = open(gi2go_ofn, "w")
    m8_gi2go = []
    skipped_n = 0
    processed_n = 0
    OUT.write("#read_id\tsubject_id\tgi\tGO\n")
    for l in IN:
        l = l.split("\t")
        # Extract the GI from the current line
        gi = int(l[1].split("|")[1])
        go=""
        
        try:
            go = gi2go_map[gi]
        except:
            skipped_n += 1
        processed_n += 1
        m8_gi2go.append([l[0], l[1], gi, go])
        OUT.write("\t".join(map(str, [l[0], l[1], gi, go])) + "\n")

    OUT.close()
    IN.close()
    
    print("Processed=" + str(processed_n) + " (" + str(skipped_n) + " skipped)")
    
    return m8_gi2go



"""
import glob
import process_m8

m8_fns = glob.glob("*nr.m8")
for m8_fn in m8_fns:
    process_m8.merge_gi2go_map(m8_fn+".map", m8_fn+".gi2go", m8_fn + ".gi2go+map", dump_read_id=True)



m8_fns = glob.glob("*.m8")
for m8_fn in m8_fns:
    process_m8.merge_gi2go_map(m8_fn+".map", m8_fn+".gi2go", m8_fn + ".gi2go+map", dump_read_id=False)

"""
## Assume gi2go_fn does not have header line
def merge_gi2go_map(map_fn, gi2go_fn, out_fn, dump_read_id=False):

    IN_map = open(map_fn)
    IN_gi2go = open(gi2go_fn)
    OUT = open(out_fn, "w")
    if dump_read_id:
        dump_out_fn = out_fn + ".id.dump"
        OUT_dump = open(dump_out_fn, "w")
    
    # Wash the comment line from map_fn
    flush = IN_map.readline()
    flush = IN_gi2go.readline()
    
    print("Processing " + map_fn + " and " + gi2go_fn)
    
    if dump_read_id:
        OUT.write("\t".join(["#line_id", "gi", "go", "tax_id", "seed_id"]) + "\n")
        OUT_dump.write("\t".join(["#line_id", "read_id", "subject_id"]) + "\n")
        line_id = 0
        for map_l, gi_l in izip(IN_map, IN_gi2go):
             map_l = map_l.rstrip("\n").split("\t")
             gi_l = gi_l.rstrip("\n").split("\t")
             
             if gi_l[2].startswith("gi|"):
                 gi_l[2] = gi_l[2].split("|")[1]
                 
             if map_l[1] != gi_l[2]:
                 print("Map and Gi2GO files are not synchronized, abort.")
                 break
             line_id += 1
             OUT_dump.write(str(line_id) + "\t" + "\t".join(gi_l[1:2]) + "\n")
             OUT.write(str(line_id)+ "\t" + "\t".join(gi_l[2:] + map_l[2:4]) + "\n")
    else:   
        OUT.write("\t".join(["#read_id", "subject_id", "gi", "go", "tax_id", "seed_id"]) + "\n")
        for map_l, gi_l in izip(IN_map, IN_gi2go):
             map_l = map_l.rstrip("\n").split("\t")
             gi_l = gi_l.rstrip("\n").split("\t")
             
             if gi_l[2].startswith("gi|"):
                 gi_l[2] = gi_l[2].split("|")[1]
                 
             if map_l[1] != gi_l[2]:
                 print("Map and Gi2GO files are not synchronized, abort.")
                 break
             
             OUT.write("\t".join(gi_l + map_l[2:4]) + "\n")
    
    IN_map.close()
    IN_gi2go.close()
    OUT.close()
    if dump_read_id:
        OUT_dump.close()
    



"""
Given the gi2go_fn, this function will extract the specified functional trait (GO, SEED, PFAM or COG)

Usage:
import process_m8
import glob
import os

gi2go_fns = glob.glob("*.gi2go+map")
taxid2lineage_map = process_m8.import_taxid2lineage()
#sample_ids = [(os.path.basename(f)).split(".")[0] for f in gi2go_fns]
[sample_list, sample_trait_list, sample_ids, tax_ids, trait_ids, cluster_stats] = process_m8.convert_gi2go_to_FDcompatible(gi2go_fns, taxid2lineage_map=taxid2lineage_map, tax_col_id="tax_id", trait_col_id="go")

#[sample_list, sample_trait_list, sample_ids, tax_ids, trait_ids, cluster_stats] = process_m8.convert_gi2go_to_FDcompatible(gi2go_fns, tax_col_id="tax_id", trait_col_id="go", sample_ids=sample_ids)

"""
def convert_gi2go_to_FDcompatible(gi2go_fns, tax_col_id, trait_col_id, taxids_clusters=None, taxid2lineage_map=None, excluding_zero=False, tax_val_delim=None, sample_ids=None, ofn_prefix=None, sample_ofn=None, trait_ofn=None, sample_trait_ofn=None, trait_val_delim="; ", missing_value="NA", disable_output=False):
    import os
    
    if sample_ids is None:
        sample_ids = {os.path.splitext((os.path.basename(fn)))[0]:fn for fn in gi2go_fns}
    
    if taxid2lineage_map is not None:
        print(">> TaxIDs will be mapped to provided lineage table (" + str(len(taxid2lineage_map)) +" lineages) <<")
    
    print(str(len(sample_ids)) + " ids found:")
    print("-----------------------------------\n" + "\n".join(sample_ids) + "\n")
    
    if ofn_prefix is None:
        ofn_prefix = "samples"
        if taxids_clusters is not None:
            ofn_prefix = ofn_prefix + "-clustered"
        
    if sample_ofn is None:
        sample_ofn = ofn_prefix + "-" + tax_col_id + "+" + trait_col_id + ".samples"
    if sample_trait_ofn is None:
        sample_trait_ofn = ofn_prefix + "-" + tax_col_id + "+" + trait_col_id + ".samples.trait"
    if trait_ofn is None:
        trait_ofn = ofn_prefix + "-" + tax_col_id + "+" + trait_col_id + ".traits"
            
    # Sample_list format:
    # RIDS:CID       tax_id_1    tax_id_2      ...
    # sample_id_1    1            0            ...
    # sample_id_2    3            12           ...
    # ...            ...          ...          ...
    sample_list = {}
    
    # Tax_list format:
    # RID:CID     trait_id_1    trait_id_2     ...
    # tax_id_1    10            21             ...
    # tax_id_2    1             400            ...
    # ...         ...           ...            ...
    trait_list = {}
    # Trait list for each sample
    sample_trait_list = {}   
    
    
    tax_ids = []
    trait_ids = []

    # Summary of clusters
    cluster_stats = {}
    if taxids_clusters is not None:
        for tid in taxids_clusters.keys():
            cluster_id = taxids_clusters[tid]
            try:
                cluster_stats[cluster_id][tid] = 0
            except:
                cluster_stats[cluster_id] = {}
                cluster_stats[cluster_id][tid] = 0
                
    
    # Iterate all the samples
    for sample_id in sample_ids.keys():
        print("Processing " + sample_id)
        
        gi2go_fn = sample_ids[sample_id]
        
        # Read the infile
        IN = open(gi2go_fn)
        header = IN.readline()
        
        # Remove comment and split column ids
        if not header.startswith("#"):
            print("Either header line is missing or does not start with '#', " + gi2go_fn + ",  skipped.")
            continue
        
        header = header.replace("#", "").split("\t")
        
        # Search if tax_col_id exists
        try:
            tax_col_idx = header.index(tax_col_id)
        except:
            print("Tax column ID not found " + gi2go_fn + ": " + tax_col_id + ", file skipped.")
            continue
        
        # Search if trait_col_id exists
        try:
            trait_col_idx = header.index(trait_col_id)
        except:
            print("Trait column ID not found in " + gi2go_fn + ": " + trait_col_id + ", file skipped.")
            continue
        
        # Initialize lists for the current sample_id
        sample_list[sample_id] = {}
        sample_trait_list[sample_id] = {}
        
        skipped_n = 0
        processed_n = 0
        # Proces the content
        for line in IN:
            #if line.startswith("#"):
            #    continue
            line = line.split("\t")
            
            tax_val = int(line[tax_col_idx])
            trait_vals = line[trait_col_idx].split(trait_val_delim)
            
            # Reassign the taxids according to specified cluster
            if taxids_clusters is not None:
                try:
                    cluster_id = taxids_clusters[tax_val]
                    cluster_stats[cluster_id][tax_val] += 1
                except:
                    skipped_n += 1
                    continue
                # Reassign taxid to cluster_id
                tax_val = cluster_id
            
            processed_n += 1
            
            # Skip if nothing presents in the tax value column or trait column
            #if len(tax_val) == 0 or len(trait_vals) == 0 or (len(trait_vals) == 1 and len(trait_vals[0]) == 0):
            #if len(tax_val) == 0 or len(trait_vals) == 0:
            if len(trait_vals) == 0:
                skipped_n += 1
                continue
            
            # Adding the tax val
            try:
                sample_list[sample_id][tax_val] += 1
            except:
                sample_list[sample_id][tax_val] = 1
            
            # Adding traits to trait tables
            for trait_val in trait_vals:
                # Handling the case that the trait value is empty
                if len(trait_val) == 0:
                    trait_val = missing_value
            
                # Adding the trait to the sample trait table    
                try:
                    sample_trait_list[sample_id][trait_val] += 1
                except:
                    sample_trait_list[sample_id][trait_val] = 1
            
                # Adding the trait to the general trait table
                try:
                    trait_list[tax_val][trait_val] += 1
                except:
                    try:
                        trait_list[tax_val][trait_val] = 0
                    except:
                        trait_list[tax_val] = {}
                    trait_list[tax_val][trait_val] = 1
        
        print("Sample " + sample_id + ": " + str(processed_n) + " (" + str(skipped_n) + " skipped)")
        
        # Append tax and trait ids to lists
        tax_ids.extend(sample_list[sample_id].keys())
        trait_ids.extend(sample_trait_list[sample_id].keys())
        
        IN.close()
        
    # Non-redundant list
    trait_ids = list(set(trait_ids))
    tax_ids = list(set(tax_ids))
    
    print("Total number of traits: " + str(len(trait_ids)))
    print("Total number of taxa: " + str(len(tax_ids)))
    
    if disable_output:
        return [sample_list, sample_trait_list, sample_ids, tax_ids, trait_ids, cluster_stats]
    
    # Filter cluster by their counts
    # Obtain totals of each cluster
    
    
    ###########################  
    # Export the sample table
    ###########################
    print("Exporting sample table to " + sample_ofn)
    except_n = 0
    skipped_mapped_id = 0
    with open(sample_ofn, "w") as OUT:
        # Map the taxid
        if taxid2lineage_map is not None:
            mapped_tax_ids = []
            for tax_id in tax_ids:
                mapped_tax_id = str(tax_id)
                try:
                    mapped_tax_id = mapped_tax_id + ":" + taxid2lineage_map[tax_id]
                except:
                    skipped_mapped_id += 1
                mapped_tax_ids.append(mapped_tax_id)
            OUT.write("\t".join(["#sample_id:"+tax_col_id] + mapped_tax_ids) + "\n")                
        else:
            OUT.write("\t".join(["#sample_id:"+tax_col_id] + map(str,tax_ids)) + "\n")
        
        for sample_id in sample_ids:
            tax_vals = [0 for i in tax_ids]
            for i, tax_id in enumerate(tax_ids):
                processed_n += 1
                try:
                    #tax_vals[i] = len(sample_list[sample_id][tax_id])
                    tax_vals[i] = sample_list[sample_id][tax_id]
                except:
                    except_n += 1
            OUT.write("\t".join([sample_id] + map(str, tax_vals)) + "\n")
    print("Processed: " + str(processed_n) + ", skipped: " + str(except_n))
    
    
    ###########################
    # Export the sample trait table
    ###########################  
    print("Exporting sample specific trait table to " + sample_trait_ofn)
    except_n = 0
    processed_n = 0
    with open(sample_trait_ofn, "w") as OUT:
        OUT.write("\t".join(["#sample_id", trait_col_id, "count"]) + "\n")
        for sample_id in sample_ids:
            for trait_id in trait_ids:
                processed_n += 1
                trait_n = 0
                try:
                    #trait_n = len(trait_list[sample_id][trait_val])
                    trait_n = sample_trait_list[sample_id][trait_id]
                except:
                    except_n += 1
                 
                if excluding_zero and trait_n == 0:
                    continue
                 
                OUT.write("\t".join([sample_id, trait_id, str(trait_n)]) + "\n")
    print("Processed: " + str(processed_n) + ", skipped: " + str(except_n))
     
     
    ###########################
    # Export the general trait table
    ###########################  
    print("Exporting tax specific trait table to " + trait_ofn)
    except_n = 0
    processed_n = 0
    with open(trait_ofn, "w") as OUT:
        OUT.write("\t".join(["#tax_id:" + trait_col_id] + trait_ids) + "\n")
        for tax_id in tax_ids:
            trait_ns = [0 for t in trait_ids]
            for i, trait_id in enumerate(trait_ids):
                processed_n += 1
                 
                try:
                    trait_ns[i] = trait_list[tax_id][trait_id]
                except:
                    except_n += 1
            
            # Map the taxid
            if taxid2lineage_map is not None:
                mapped_tax_id = str(tax_id)
                try:
                    mapped_tax_id = mapped_tax_id + ":" + taxid2lineage_map[tax_id]
                except:
                    skipped_mapped_id += 1
                OUT.write("\t".join([mapped_tax_id] + map(str, trait_ns)) + "\n")    
            else:
                OUT.write("\t".join(map(str, [tax_id] + trait_ns)) + "\n")
            
            #OUT.write("\t".join([tax_id] + map(str, trait_ns)) + "\n")
                 
    print("Processed: " + str(processed_n) + ", skipped: " + str(except_n))    
    
#    return [sample_list, sample_trait_list, trait_list, sample_ids, tax_ids, trait_ids]
    return [sample_list, sample_trait_list, sample_ids, tax_ids, trait_ids, cluster_stats]


    

# def add_pfam_to_gi2go(gi2go_fn, go2pfam_map=None, out_fn=None, dump_read_id=True):
#     if out_fn is None:
#         out_fn = gi2go_fn + "+pfam"
#     
#     if go2pfam_map is None:
#         go2pfam_map = import_pfam2go()
#     
#     IN_gi2go = open(gi2go_fn)
#     OUT = open(out_fn, "w")
#     
#     print("Appending Pfam data to " + gi2go_fn + " and exporting to " + out_fn)
#     
#     # Wash the comment line from map_fn
#     nomatch_n = 0
#     processed_n = 0
#     
#     OUT.write("\t".join(["#read_id", "subject_id", "gi", "go", "tax_id", "seed_id", "pfam"]) + "\n")
#     for gi_l in IN_gi2go:
#         gi_l = gi_l.split("\t")
#         gos = gi_l[3].split("; ")
#         
#         pfam = []
#         for go in gos:
#             try:
#                 pfam.extend(list(go2pfam_map[go]["Pfam"].keys()))
#                 processed_n += 1
#             except:
#                 nomatch_n += 1
#                 
#         OUT.write("\t".join(gi_l + [";".join(pfam)]) + "\n")
# 
#     print("Processed: " + str(processed_n) + ", no match="+str(nomatch_n))
#     
#     IN_gi2go.close()
#     OUT.close()
   


"""
This function prepares a map for mapping Pfam Accession ID to related GO terms. This function 
returns a map linking Pfam to GO, which can be used as a function argument i
"""
def import_pfam2go(pfam2go_fn="/home/siukinng/db/Markers/GeneOntology/pfam2go"):
    print("Importing from " + pfam2go_fn)
    with open(pfam2go_fn) as IN:
         pfam2go_map = IN.read().splitlines()
    # Wash header lines
    pfam2go_map = [(p.split(" > ")[0]).split(" ") + (p.split(" > ")[1]).split(" ; ") for p in pfam2go_map if not p.startswith("!")]
    
    # Construct the go2pfam_map
    go2pfam_map = {}
    duplicated_n = 0
    processed_n = 0
    for p in pfam2go_map:
        go = p[3]
        pfam = p[0].replace("Pfam:", "")
        pfam_name = p[1]
        go_ctx = p[2].replace("GO:", "")
        try:
            go2pfam_map[go]["Pfam"][pfam] = pfam_name
            #go2pfam_map[go]["Pfam_Output"].append(pfam)
        except:
            go2pfam_map[go] = {}
            go2pfam_map[go]["GO"] = go_ctx
            go2pfam_map[go]["Pfam"] = {}
            go2pfam_map[go]["Pfam"][pfam] = pfam_name
            #go2pfam_map[go]["Pfam_Output"] = [pfam]
            
            
        processed_n += 1
        
    print("Number of GO processed: " + str(processed_n) + " (" + str(duplicated_n) + " duplicated)")
    
    return go2pfam_map
        
   
   
"""

By providing a list of taxids, this function will search theirs taxonomical lineages from taxID to
lineage map (taxid2lineage_map) and collapse the lineage to the specified taxonomic level (tax_level). 
The taxids sharing a same lineage are clustered together.


Usage: 

import process_m8
import glob
import os

gi2go_fns = glob.glob("*.gi2go+map")

taxid2lineage_map = import_taxid2lineage()
[sample_list, sample_trait_list, sample_ids, tax_ids, trait_ids, cluster_stats] = process_m8.convert_gi2go_to_FDcompatible(gi2go_fns, "tax_id", "go")
#[sample_list, sample_trait_list, sample_ids, tax_ids, trait_ids, cluster_stats] = process_m8.convert_gi2go_to_FDcompatible(gi2go_fns, "tax_id", "go", disable_output=True)
[cluster_ids, cluster2taxid, taxid2cluster] = process_m8.cluster_taxids_by_tax_lineage(tax_ids)
[sample_list, trait_list, sample_ids, tax_ids, trait_ids, cluster_stats] = process_m8.convert_gi2go_to_FDcompatible(gi2go_fns, "tax_id", "go", taxids_clusters=taxid2cluster)

"""
def cluster_taxids_by_tax_lineage(taxids, taxid2lineage_map=None, tax_level="g", excluding_missing_value=False, missing_value_label="Unassigned"):
    print("Processing " + str(len(taxids)) + " taxid using tax_level: " + tax_level)
    
    # select tax_level splitter
    if tax_level == "s":
        tax_level_tag = "?"
    elif tax_level == "g":
        tax_level_tag = ";s__"
        tax_level_tag_end = ";s__"
    elif tax_level == "f":
        tax_level_tag = ";g__"
        tax_level_tag_end = ";g__;s__"
    elif tax_level == "o":
        tax_level_tag = ";f__"
        tax_level_tag_end = ";f__;g__;s__"
    elif tax_level == "c":
        tax_level_tag = ";o__"
        tax_level_tag_end = ";o__;f__;g__;s__"
    elif tax_level == "p":
        tax_level_tag = ";c__"
        tax_level_tag_end = ";c__;o__;f__;g__;s__"
    elif tax_level == "k":
        tax_level_tag = ";p__"
        tax_level_tag_end = ";p__;c__;o__;f__;g__;s__"
    else:
        print("Unknown taxa level: " + tax_level)
        return [None, None, None]
             
    # Convert taxids into integer for efficient processing
    taxids = map(int, taxids)
    
    #tax_level_tag = tax_level + "__"
    
    # If taxids sharing a same tax_level are clustered
    taxid2cluster = {}
    cluster2taxid = {}
    cluster_ids = []
    if taxid2lineage_map is None:
        taxid2lineage_map = import_taxid2lineage()
    
    
    # Reverse map:
    lineage2taxid_map = {v : k for k, v in taxid2lineage_map.items()}
    
    
    print("Clustering")
    skipped_n = 0
    processed_n = 0
    unassigned_n = 0
    missing_n = 0
    orphan_n = 0
    for taxid in taxids:
        processed_n += 1
        try:
            lineage = taxid2lineage_map[taxid]
        except:
            skipped_n += 1
            continue
        
        # Extract the value at specified tax level
        #tax_level_val = [l for l in lineage.split(";") if l.startswith(tax_level_tag)]
        tax_level_val = lineage.split(tax_level_tag)
        
        if lineage.find(tax_level_tag) == -1:
            tax_level_val = missing_value_label
            missing_n += 1
        
        # tax value not available, so marked "Unassigned"
        if len(tax_level_val) == 0:
            tax_level_val = missing_value_label
            missing_n += 1

        # Extract the tax value
        tax_level_val = tax_level_val[0]
        
        # tax value goes empty, also marked "Unassigned"
        if len(tax_level_val.replace(tax_level_tag, "")) == 0:
            tax_level_val = missing_value_label
            unassigned_n += 1

        rev_tax_id = -1
        try:
            rev_tax_id = lineage2taxid_map[tax_level_val+tax_level_tag_end]
        except:
            orphan_n += 1

        tax_level_val = str(rev_tax_id) + ":"+ tax_level_val

        taxid2cluster[taxid] = tax_level_val
        try:
            cluster2taxid[tax_level_val].append(taxid)
        except:
            cluster2taxid[tax_level_val] = []
            cluster2taxid[tax_level_val].append(taxid)
            cluster_ids.append(tax_level_val)
    

    print("Processed: " + str(processed_n) + ", unassigned: " + str(unassigned_n) + ", missing: " + str(missing_n) + ", orphan: " + str(orphan_n)  + ", skipped (lineage information not available): " + str(skipped_n))
    print("Number of cluster: " + str(len(cluster2taxid)))
     
    return [cluster_ids, cluster2taxid, taxid2cluster]
    
    
 

"""
import the taxIDs to lineage map.

"""
def import_taxid2lineage(taxid2lineage_fn="/home/siukinng/db/Markers/ncbi_nr/mapping/taxid2tax_lineage.map"):
    print("Importing taxid to lineage map from " + taxid2lineage_fn)
    with open(taxid2lineage_fn) as IN:
        taxid2lineage_map = IN.read().splitlines()
    
    if taxid2lineage_map[0].startswith("#"):
        del taxid2lineage_map[0]
    
    taxid2lineage_map = [t.split("\t") for t in taxid2lineage_map]
    taxid2lineage_map = {int(t[0]):t[1] for t in taxid2lineage_map}
    
    print(str(len(taxid2lineage_map)) + " row imported.")
    
    return taxid2lineage_map



"""


Usage:

import process_m8
sample_fn = 'samples-tax_id+go.samples'

process_m8.cluster_sample_fn_otu(sample_fn)

"""
def cluster_sample_fn_otu(sample_fn, tax_level="g", sample_ofn=None):
    if sample_ofn is None:
        sample_ofn = sample_fn + "." +  tax_level + ".clustered"
    
    print("Processing " + sample_fn + " and results will be exported to " + sample_ofn)
    
    with open(sample_fn) as IN:
        otus = IN.read().splitlines()
    
    header_label = otus[0].split("\t")[0] 
    header = otus[0].split("\t")[1:]
    del otus[0]
    
    taxids = [h.split(":")[0] for h in header]
    taxid_idx = {int(t):i+1 for i,t in enumerate(taxids)}
    
    # Expand the tab format
    otus = [o.split("\t") for o in otus]
    
    sample_ids = [o[0] for o in otus]
    print("Number of sample IDs: " + str(len(sample_ids)))
    
    [cluster_ids, cluster2taxid, taxid2cluster] = cluster_taxids_by_tax_lineage(taxids, tax_level=tax_level)
    print("Number of clusters: " + str(len(cluster_ids)))
    
    # Check list for what has been exported
    check_list = [0 for t in taxids]
    
    OUT = open(sample_ofn, "w")
    
    processed_n = 0
    OUT.write("\t".join([header_label] + cluster_ids) + "\n")
    for i, sample_id in enumerate(sample_ids):
        otu_vals = [sample_id]
        print("Extracting " + sample_id)
        for cluster_id in cluster_ids:
            cluster_val = 0
            cluster_taxids = cluster2taxid[cluster_id]
            for tid in cluster_taxids:
                idx = taxid_idx[tid]
                cluster_val += int(otus[i][idx])
                processed_n += 1
            otu_vals.append(cluster_val)
        OUT.write("\t".join(map(str, otu_vals)) + "\n")
    OUT.close()
    
    print("Number of data processed: " + str(processed_n)) 



"""
import process_m8
import glob

taxid2lineage_map = process_m8.import_taxid2lineage()
gi2go_map_fns = glob.glob("*.gi2go+map")

for gi2go_map_fn in gi2go_map_fns:
    process_m8.map_lineage2gi2go_map_fn(gi2go_map_fn, taxid2lineage_map=taxid2lineage_map)


"""
def map_lineage2gi2go_map_fn(gi2go_map_fn, gi2go_map_ofn=None, taxid2lineage_map=None, taxid_tag="tax_id"):
    print("Processing " + gi2go_map_fn)
    if taxid2lineage_map is None:
        taxid2lineage_map = import_taxid2lineage()
    
    if gi2go_map_ofn is None:
        gi2go_map_ofn = gi2go_map_fn + "+lineage"
    
    IN = open(gi2go_map_fn)
    OUT = open(gi2go_map_ofn, "w")
    
    header = IN.readline()
    try:
        taxid_col_idx = header.split("\t").index(taxid_tag)
    except:
        raise Exception("Tax ID column is not defined: " + taxid_tag + ", abort now.")

    OUT.write(header.rstrip() + "\tlineage\n")
    processed_n = 0
    skipped_n = 0
    # Go through every single line
    for l in IN:
        l = l.rstrip()
        processed_n += 1
        lineage = "."
        try:
            taxid = int(l.split("\t")[taxid_col_idx])
            lineage = taxid2lineage_map[taxid]
        except:
            skipped_n += 1
        OUT.write(l + "\t" + lineage + "\n")
    
    print("Number of processed: " + str(processed_n))
    print("Number of skipped: " + str(skipped_n))
    
    
    IN.close()
    OUT.close()

   

"""

cd /disk/rdisk08/siukinng/samples/lab/m8/scaffolds/SEED


import process_m8
import glob


[peg2function_map, SEED_families2func_map, fig2SEED_families_map, asserted_fig2SEED_families_map, subsystem2role_map] = process_m8.import_seed_data() 
m8_fns = glob.glob("*SEED.m8")
for m8_fn in m8_fns:
    process_m8.process_SEED(m8_fn,peg2function_map, SEED_families2func_map, fig2SEED_families_map, asserted_fig2SEED_families_map, subsystem2role_map)
    
"""
def process_SEED(m8_fn, peg2function_map, SEED_families2func_map, fig2SEED_families_map, asserted_fig2SEED_families_map, subsystem2role_map, map_ofn=None, remove_redundant_qid=False):
#def process_SEED(m8_fn, map_ofn=None, remove_redundant_qid=False, assigned_function_fn="/home/siukinng/db/Markers/SEED/Release70/assigned_functions.txt", SEED_families2func_fn="/home/siukinng/db/Markers/SEED/Release70/fam.func.index", fig2SEED_families_fn="/home/siukinng/db/Markers/SEED/Release70/families.2c", expert_assertion_fn="/home/siukinng/db/Markers/SEED/ach_expert_assertions", subsystem2role_fn="/home/siukinng/db/Markers/SEED/subsystems2role"):
    print("Processing " + m8_fn) 
    
    #
    #ids1 = list(fig2SEED_families_map.keys())
    #ids2 = list(asserted_fig2SEED_families_map.keys())
    
    
    if map_ofn is None:
        map_ofn = m8_fn + ".mapped2SEED"
    
    IN = open(m8_fn)
    OUT = open(map_ofn, "w")
    
    OUT.write("\t".join(["#SEQ_ID", "PEG", "SCORE", "FUNC_ROLE", "SUBSYSTEM", "SYSTEM", "FIGFAM_ID", ]) + "\n")
    processed_n = 0
    skipped_n = 0
    id2seed_map = []
    for row in IN:
        row = (row.rstrip()).split("\t")
        qid = row[0]
        peg = row[1]
        #score = float(row[11])
        score = row[11]
        row_dat = [qid, peg, score, ".", ".", ".", "."]
        try:
            row_dat[3] = peg2function_map[peg]          # functional role
            row_dat[6] = fig2SEED_families_map[peg]     # Figfam_id
            roles = subsystem2role_map[row_dat[3]]            
            row_dat[4] = roles[2]                       # Subsystem
            row_dat[5] = roles[1]                       # System
            
            #func = SEED_families2func_map[fam_id]
            #row_dat[2:] = [fam_id, func, score]
        except:
            skipped_n += 1
        processed_n += 1
        id2seed_map.append(row_dat)
        OUT.write("\t".join(row_dat) + "\n")

    IN.close()
    OUT.close()
    print("Number of processed: " + str(processed_n))
    print("Number of skipped: " + str(skipped_n))



"""
import process_m8
import glob

mapped2SEED_fns = glob.glob("*.mapped2SEED")
for mapped2SEED_fn in mapped2SEED_fns:
    process_m8.filter_mapped2SEED_with_subsystem(mapped2SEED_fn)
    
"""
def filter_mapped2SEED_with_subsystem(mapped2SEED_fn, mapped2SEED_ofn=None):
    if mapped2SEED_ofn is None:
        mapped2SEED_ofn = mapped2SEED_fn + ".filtered"    
    
    print("Processing "+ mapped2SEED_fn + " and export to " + mapped2SEED_ofn)
    
    discard_n = 0
    processed_n = 0
    IN = open(mapped2SEED_fn)
    OUT = open(mapped2SEED_ofn, "w")
    
    for l in IN:
        sl = l.split("\t")
        if sl[4] != ".":
            OUT.write(l)
        else:
            discard_n += 1
        processed_n += 1
    
    IN.close()
    OUT.close()
    print("Number of processed: " + str(processed_n))
    print("Number of discarded: " + str(discard_n))



"""
import process_m8
import glob
import gc

cd ~/MG/m8/scaffolds/NR
# inside the directory: ~/MG/m8/scaffolds/NR
gi2go_fns = glob.glob("S*.gi2go+map+lineage")
for gi2go_fn in gi2go_fns:
    print("Processing " + gi2go_fn)
    SEED_dir = "../SEED/"
    read_1_fn = gi2go_fn.replace("+nr.m8.gi2go+map+lineage", "_1.trimmed+SEED.m8.mapped2SEED.filtered")
    read_2_fn = gi2go_fn.replace("+nr.m8.gi2go+map+lineage", "_2.trimmed+SEED.m8.mapped2SEED.filtered")
    print(read_1_fn + " and " + read_2_fn)
    process_m8.merge_mapped2SEED_gi2go_map(SEED_dir + read_1_fn, gi2go_fn, "../" + read_1_fn + "+gi2go+map+lineage") 
    process_m8.merge_mapped2SEED_gi2go_map(SEED_dir + read_2_fn, gi2go_fn, "../" + read_2_fn + "+gi2go+map+lineage")
    gc.collect()
    #%reset  

"""
def merge_mapped2SEED_gi2go_map(mapped2SEED_fn, gi2go_map_fn, merge_ofn, operation="intersect"):
    merge_data = {}
    
    processed_n = 0
    print("Processing " + gi2go_map_fn)
    IN = open(gi2go_map_fn)
    gi2go_header = IN.readline().rstrip()
    for l in IN:
        sl = l.replace("\n", "").split("\t")
        read_id = sl[0]
        
        merge_data[read_id] = [sl]
        processed_n += 1
    IN.close()
    print("Number of processed: " + str(processed_n))
    
    
    processed_n = 0
    print("Processing " + mapped2SEED_fn)
    IN = open(mapped2SEED_fn)
    mapped2SEED_header = IN.readline().rstrip()
    for l in IN:
        sl = l.replace("\n", "").split("\t")
        read_id = sl[0]
        try:
            merge_data[read_id].append(sl[1:])
        except:
            merge_data[read_id] = [sl[1:]]
        processed_n += 1
    IN.close()
    print("Number of processed: " + str(processed_n))

    print("Exporting data found in both " + mapped2SEED_fn + " " + gi2go_map_fn + " (Intersection operation)")
    skipped_n = 0
    processed_n = 0
    exported_n = 0
    OUT = open(merge_ofn, "w")
    # Print header
    OUT.write(gi2go_header+"\t"+"\t".join(mapped2SEED_header.split("\t")[1:])+"\n")
    for read_id in merge_data.keys():
        if len(merge_data[read_id]) == 2:
            OUT.write("\t".join(merge_data[read_id][0] + merge_data[read_id][1]) + "\n")
            exported_n += 1
        else:
            skipped_n += 1
        processed_n += 1
    OUT.close()
    print("Number of processed: " + str(processed_n))
    print("Number of exported: " + str(exported_n))
    print("Number of skipped: " + str(skipped_n))
    
    del merge_data
    gc.collect()
    


"""

"""
def import_seed_data(assigned_function_fn="/home/siukinng/db/Markers/SEED/Release70/assigned_functions.txt", SEED_families2func_fn="/home/siukinng/db/Markers/SEED/Release70/fam.func.index", fig2SEED_families_fn="/home/siukinng/db/Markers/SEED/Release70/families.2c", expert_assertion_fn="/home/siukinng/db/Markers/SEED/ach_expert_assertions", subsystem2role_fn="/home/siukinng/db/Markers/SEED/subsystems2role"):
    print("Reading table of PEG's functional role from " + assigned_function_fn)
    with open(assigned_function_fn) as IN:
        peg2function_map = IN.read().splitlines()
    peg2function_map = {s.split("\t")[0] : s.split("\t")[1] for s in peg2function_map}
    print(str(len(peg2function_map)) + " imported.")
    
    print("Reading SEED family function descriptor from " + SEED_families2func_fn)
    with open(SEED_families2func_fn) as IN:
        SEED_families2func_map = IN.read().splitlines()
    SEED_families2func_map = {s.split("\t")[1] : s.split("\t")[2] for s in SEED_families2func_map}
    print(str(len(SEED_families2func_map)) + " imported.")
    
    print("Reading FIG_ID to SEED family map from " + fig2SEED_families_fn)
    with open(fig2SEED_families_fn) as IN:
        fig2SEED_families_map = IN.read().splitlines()
    fig2SEED_families_map = {s.split("\t")[1] : s.split("\t")[0] for s in fig2SEED_families_map}
    print(str(len(fig2SEED_families_map)) + " imported.")
    
    print("Reading expert asserted data from " + expert_assertion_fn)
    with open(expert_assertion_fn) as IN:
        asserted_fig2SEED_families_map = IN.read().splitlines()
    asserted_fig2SEED_families_map = {s.split("\t")[0] : s.split("\t")[1] for s in asserted_fig2SEED_families_map}
    print(str(len(asserted_fig2SEED_families_map)) + " imported.")
    
    # ADD additional asserted fig annotation to fig2SEED_families_map
    for fig in asserted_fig2SEED_families_map.keys():
        fig2SEED_families_map[fig] = asserted_fig2SEED_families_map[fig]    
    
    subsystem2role_fn
    print("Reading table of SEED subsystem roles from " + subsystem2role_fn)
    with open(subsystem2role_fn) as IN:
        subsystem2role_map = IN.read().splitlines()
    subsystem2role_map = {s.split("\t")[3] : s.split("\t")[0:3] for s in subsystem2role_map}
    print(str(len(asserted_fig2SEED_families_map)) + " imported.")
     
    
    return [peg2function_map, SEED_families2func_map, fig2SEED_families_map, asserted_fig2SEED_families_map, subsystem2role_map]




"""
import process_m8
import glob

cd ~/MG/m8/scaffolds

fns = glob.glob("*.mapped2SEED.filtered+gi2go+map+lineage")

merge_list = ["SWH-Cell55_Y2", "GZ-Seed", "SWH-Seed", "GZ-Xyl_Y1", "GZ-Xyl_Y2", "SWH-Xyl_Y1", "SWH-Xyl_Y2", "GZ-Cell_Y1", "GZ-Cell_Y2", "SWH-Cell_Y1", "SWH-Cell_Y2", ]

#process_m8.summary_SEED(fns, "all_samples.mapped2SEED.filtered+gi2go+map+lineage.func_role.summary", merge_list, col_idx=9)

process_m8.summary_SEED(fns, "all_samples.mapped2SEED.filtered+gi2go+map+lineage.subsystem.summary", merge_list,col_idx=10)

"""
def summary_SEED(fns, summary_ofn, merge_list=None, col_idx=10): # col_idx=10 :subsystem, col_idx=9: functional role
    sample_n = 0
    sample_ids = []
    SEED_table = {}
    
    # estimate
    if merge_list is not None:
        sample_ids = merge_list
#         for fn in fns:
#             for id in merge_list:
#                 if fn.startswith(id):
#                     sample
    else:
        sample_ids = fns
    
    
    for i, fn in enumerate(fns):
        print("Processing " + str(i) + " " + fn)
        #sample_ids.append(fn)
        if merge_list is not None:
            i = -1
            for j, id in enumerate(sample_ids):
                if os.path.basename(fn).startswith(id):
                    print(fn + " will be appeneded to " + id)
                    i = j
                    break
            if i == -1:
                print(fn + " was not selected, skipped.")
                continue
        
        IN = open(fn)
        processed_n = 0
        header = IN.readline().rstrip()
        for l in IN:
            l = l.replace("\n", "").split("\t")
            subsystem = l[col_idx]
            try:
                SEED_table[subsystem][i] += 1
            except:
                SEED_table[subsystem] = [0 for o in sample_ids]
                SEED_table[subsystem][i] = 1
            processed_n += 1
        IN.close()
        print("Number of processed: " + str(processed_n))
        
    print("Exporting results to " + summary_ofn)
    OUT = open(summary_ofn, "w")
    OUT.write("Subsystem\t" + "\t".join(sample_ids) + "\n")
    for k in SEED_table.keys():
        OUT.write(k + "\t" + "\t".join(map(str, SEED_table[k])) + "\n")
    OUT.close()
    
    del SEED_table
    gc.collect()



"""
with open("process_m8.py") as fp:
    for i, line in enumerate(fp):
        if "\xe2" in line:
            print i, repr(line)

import glob
import process_m8
summary_fns = glob.glob("*_1.trimmed+SEED.m8.mapped2SEED.filtered+gi2go+map+lineage")
data = {}
for summary_fn in summary_fns:
    summary_1_fn = summary_fn
    summary_2_fn = summary_fn.replace("_1.tr", "_2.tr")
    sample_id = summary_fn.replace("_1.trimmed+SEED.m8.mapped2SEED.filtered+gi2go+map+lineage", "")
    data[sample_id] = process_m8.summarize_lineage_summary2(summary_1_fn, summary_2_fn)


"""
def summarize_lineage_summary2(summary_1_fn, summary_2_fn, missing="Unassigned"):
    print("Reading from " + summary_1_fn + " and " + summary_2_fn)
    df = {}
    
    # Read_1
    IN = open(summary_1_fn)
    IN2 = open(summary_2_fn)
    l = IN.readline()
    l = IN2.readline()
    
    for l, l2 in izip(IN, IN2):
        #[lineage, functional_role, subsystem] = l.rstrip().split("\t")
        try:
            l = l.rstrip().split("\t")
            
            lineage = l[6].split("g__",1)[1].split(";",1)[0]
            functional_role = l[9]
            subsystem = l[10]
        #print(lineage + "xxx, " + functional_role + "xxx, " + subsystem)
        except:
            continue

        if len(lineage) == 0:
            lineage = missing
            
        #print(lineage)
        try:
            df[lineage][subsystem] += 1
        except:
            try:
                df[lineage][subsystem] = 1
            except:
                df[lineage] = {}
                df[lineage][subsystem] = 1
        
        #
        l = l2        
        try:
            l = l.rstrip().split("\t")
            
            lineage = l[6].split("g__",1)[1].split(";",1)[0]
            functional_role = l[9]
            subsystem = l[10]
        #print(lineage + "xxx, " + functional_role + "xxx, " + subsystem)
        except:
            continue

        if len(lineage) == 0:
            lineage = missing
            
        #print(lineage)
        try:
            df[lineage][subsystem] += 1
        except:
            try:
                df[lineage][subsystem] = 1
            except:
                df[lineage] = {}
                df[lineage][subsystem] = 1 
                        
    IN2.close()
    IN.close()
    
    return df



def summarize_lineage_summary(summary_fn, missing="Unassigned"):
    print("Reading from " + summary_fn)
    df = {}
    
    # Read_1
    IN = open(summary_fn)
    l = IN.readline()
    
    for l in IN:
        #[lineage, functional_role, subsystem] = l.rstrip().split("\t")
        try:
            l = l.rstrip().split("\t")
            
            #print(l[6])
            
            #print(len(l))
            lineage = l[6].split("g__",1)[1].split(";",1)[0]
            functional_role = l[9]
            subsystem = l[10]
        #print(lineage + "xxx, " + functional_role + "xxx, " + subsystem)
        except:
            continue

        if len(lineage) == 0:
            lineage = missing
            
        #print(lineage)
        try:
            df[lineage][subsystem] += 1
        except:
            try:
                df[lineage][subsystem] = 1
            except:
                df[lineage] = {}
                df[lineage][subsystem] = 1
    IN.close()
    
    
    return df


"""

# data is created by calling summarize_lineage_summary2() 
subsystems = ["Monosaccharides", "DNA repair", "Fermentation", "Glycoside hydrolase", "DNA replication", "Central carbohydrate metabolism", "One-carbon metabolism"]
for subsystem in subsystems:
    process_m8.export_functional_summary(data, subsystem)

"""
def export_functional_summary(df, functional_label, out_fn=None):
    if out_fn is None:
        out_fn = functional_label.replace(" ", "_") + ".functional_summary"
        
    mtx = {} # Row = species, col = sample_id, element = read_count
    
    sample_ids = list(df.keys()) 
    mtx["HEADER"] = sample_ids
    for i, sample_id in enumerate(sample_ids):
        sample_df = df[sample_id]
#         try:
#             df2 = sample_df[functional_label]
#         except:
#             continue
        
        species_ids = sample_df.keys()
        print(species_ids[0])
        skipped_n = 0
        for species_id in species_ids:
            val = 0
            try:
                val = sample_df[species_id][functional_label]
            except:
                skipped_n += 1
                continue
            try:
                mtx[species_id][i] = val
            except:
                mtx[species_id] = [0 for j in sample_ids]
                mtx[species_id][i] = val

    print("Skipped: " + str(skipped_n))
    
    OUT = open(out_fn, "w")
    OUT.write("#HEADER\t" +"\t".join(mtx["HEADER"]) + "\n")
    del mtx["HEADER"]
    species_ids = list(mtx.keys())
    for sample_id in sample_ids:
        for species_id in species_ids:
            OUT.write(species_id + "\t" + "\t".join(map(str, mtx[species_id])) + "\n")  
    OUT.close()



"""
import glob
import process_m8

gi2go_map_lineage_fns = glob.glob("*.gi2go+map+lineage")
for gi2go_map_lineage_fn in gi2go_map_lineage_fns:
    process_m8.assign_contig_taxonomic_lineage(gi2go_map_lineage_fn)
    
"""
def assign_contig_taxonomic_lineage(gi2go_map_lineage_fn, assign_ofn=None):
    print("Processing " + gi2go_map_lineage_fn)
    
    if assign_ofn is None:
        assign_ofn = gi2go_map_lineage_fn + ".contig_tax"
        
    
    with open(gi2go_map_lineage_fn) as IN:
        rows = IN.read().splitlines()
    rows = [r.split("\t") for r in rows]
    
    contigs = {}
    protein_ids = []
    protein_ids2lineage_map = {}
    
    skipped_n = 0
    processed_n = 0
    for r in rows:
        processed_n += 1
        lineage = ""
        try:
            protein_id = r[0]
            contig_id = protein_id[::-1].split("_", 1)[1][::-1]
            #lineage = r[6].split("g__")[1]
            lineage = r[6].split("g__")[1].split(";")[0]
        except:
            #print("Lineage: =" + r[6] + ".")
            skipped_n += 1

            
        if len(lineage) == 0:
            linege = "Unknown"
        
        try:
            contigs[contig_id].append(lineage)
        except:
            contigs[contig_id] = []
            contigs[contig_id].append(lineage)
    
    print("Processed=" + str(processed_n))
    print("Skipped=" + str(skipped_n))


    contig_ids = sorted(contigs, key=lambda k: len(contigs[k]), reverse=True)
    
    # Export
    OUT = open(assign_ofn, "w")
    OUT.write("#Contig_ID\tLineage\tTotal_Count\tLineage_Prob\n")
    for contig_id in contig_ids:
        #unique_lineages = list(set(contigs[contig_id]))
        (lineage, lineage_n) = Counter(contigs[contig_id]).most_common(1)[0]
        OUT.write(contig_id + "\t" + lineage + "\t" + str(len(contigs[contig_id])) + "\t" + str(lineage_n / len(contigs[contig_id])) + "\n")
        
    OUT.close()
    


"""
This function extracts contig sequences and group them together based on their assigned taxonomic identity.

Usage:
import process_m8

sample_ids = ["SWH-Cell_Y2"]
for sample_id in sample_ids:
    bla_fn = sample_id + "+nr.renamed.m8.gi2go+map+lineage.contig_tax"
    fa_fn = "/home/siukinng/MG/scaffolds_2000/" + sample_id + ".fa"
    process_m8.extract_contig_by_taxonomic_lineage(bla_fn, fa_fn, group_n_threshold=300)


"""
def extract_contig_by_taxonomic_lineage(contig_tax_fn, contig_fa_fn, group_fa_ofn=None, p_value=0.8, group_n_threshold=500, unassigned_label="Unassigned", unknown_label="Unknown"):
    print("Importing from " + contig_tax_fn)
    with open(contig_tax_fn) as IN:
        flush = IN.readline()
        contig_tax_map = IN.read().splitlines()
    contig_tax_map = {c.split("\t")[0]:c.split("\t")[1:] for c in contig_tax_map}
    
    tax_group = {}
    tax_group_size = {}
    
    #print(contig_tax_map[contig_tax_map.keys()[0]])
            
    # 
    for contig_id in contig_tax_map.keys():
        tax = contig_tax_map[contig_id][0]
        #print(tax)
        if len(tax) == 0:
            tax = unknown_label
            
        try:
            tax_group[tax].append(contig_id)
            tax_group_size[tax] += len(contig_id)
        except:
            tax_group[tax] = []
            tax_group[tax].append(contig_id)
            tax_group_size[tax] = 0
            tax_group_size[tax] += len(contig_id)
    
    #print(",".join(tax_group_size.keys()))
    
    tax_group_ids = sorted(tax_group_size, key=lambda x:tax_group_size[x])
    
    print("Number of tax group: " + str(len(tax_group)))
    print("Number of tax group larger than " +str(group_n_threshold) + ": " +str(len([1 for t in tax_group_size.keys() if tax_group_size[t] > group_n_threshold])))
     
    for tax_group_id in tax_group_ids:
        #tax_id = tax_group_size.keys()[tax_group_id]
        if tax_group_size[tax_group_id] > group_n_threshold:
            print(tax_group_id + ": " + str(tax_group_size[tax_group_id]))
    


"""
# Metagenome normalization
# http://2014-5-metagenomics-workshop.readthedocs.org/en/latest/annotation/normalization.html
# 1. (counts of gene X / total number of reads) * 1000000
# 2. (counts of gene X / counts of 16S rRNA gene)
# 3. (counts of gene X / total number of mapped reads)


hmm_orf_dict = mg_pipeline.postprocess_HMMER_search_by_fn("all_samples.renamed+Pfam.dom.tbl", hmm_score_threshold=10, hmm_tc_fn="/home/siukinng/db/Markers/Pfam/Pfam.tc")

hmm_dom_tbl = mg_pipeline.generate_dom_tbl(hmm_orf_dict, hmm_accession_as_key=False)




subsystems = read.table("all_samples.mapped2SEED.filtered+gi2go+map+lineage.subsystem.summary", sep="\t", header=T, row.name=1, stringsAsFactors=F)
subsystems <- subsystems[, -1]

# Normalize
for(i in 1 : nrow(subsystems))
{
subsystems[i, ] <- subsystems[i,] / colSums(subsystems)
}

grouping <- c(1,1,0,0,0,0,0,0,0,0)
p_vals <- rep(-1, nrow(subsystems))
changes <- rep(0, nrow(subsystems))
for(i in 1 : nrow(subsystems))
{
    v <- wilcox.test(as.numeric(subsystems[i,]) ~ grouping, alternative = "less")
    p_vals[i] <- v$p.value
    changes[i] <- mean(as.numeric(subsystems[i,!grouping])) / mean(as.numeric(subsystems[i,grouping]))
}
selected_idx <- which(p_vals < 0.05)
paste(rownames(subsystems)[selected_idx], "=", p_vals[selected_idx], ", change=", changes[selected_idx], sep="")



p_vals <- rep(-1, nrow(subsystems))
changes <- rep(0, nrow(subsystems))
for(i in 1 : nrow(subsystems))
{
    v <- wilcox.test(as.numeric(subsystems[i,]) ~ grouping, alternative = "less")
    p_vals[i] <- v$p.value
    changes[i] <- mean(as.numeric(subsystems[i,!grouping])) / mean(as.numeric(subsystems[i,grouping]))
}
selected_idx <- which(p_vals < 0.05)
paste(rownames(subsystems)[selected_idx], "=", p_vals[selected_idx], ", change=", changes[selected_idx], sep="")
"""